{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c059c41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FIRAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FIRAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd324ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1549, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=pd.read_excel('C:/Users/FIRAS/Desktop/Ref NLP/work/dataset_marrocain.xlsx')\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af88e98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                             Commentaire   Classe\n",
       "0      محند اعطيك الاختيارات ههههه و انت احسن الظن ف...   normal\n",
       "1        إبراهيم الجعفري رئيس وزراء العراق إلتون جون...  abusive\n",
       "2      لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...   normal\n",
       "3     تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...   normal\n",
       "4       مكنهدرش على باغي الفلوس على ود راحة البال اه...     hate\n",
       "...                                                 ...      ...\n",
       "1544   يفكّرونَ رَبَّنَا الجنسيات والعربية للسعودين ...   normal\n",
       "1545                                     قل يامُسخّر لك   normal\n",
       "1546  تبغي تحماقغير الزمراوي فالاوزان المزيرية والقو...   normal\n",
       "1547  بقى برشيلونة بالقراءة المصنّفة بريطانيا الصهيو...   normal\n",
       "1548  لكن منتقلقوش كيتعرضوا والسجادة وتفرّقهم بروفاي...   normal\n",
       "\n",
       "[1549 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f87f970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Commentaire', 'Classe'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29d1a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1548</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1485</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>هاد ناس برشيد عالام تحية للعنصرية النسوي</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12</td>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Commentaire  Classe\n",
       "count                                        1548    1549\n",
       "unique                                       1485       3\n",
       "top     هاد ناس برشيد عالام تحية للعنصرية النسوي   normal\n",
       "freq                                           12    1361"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a25de95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Commentaire    1\n",
       "Classe         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01fcfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb25b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Commentaire    False\n",
       "Classe         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be39ebd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FIRAS\\AppData\\Local\\Temp\\ipykernel_5456\\1070480500.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['Label']=le.fit_transform(data_train['Classe'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محند اعطيك الاختيارات ههههه و انت احسن الظن ف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>إبراهيم الجعفري رئيس وزراء العراق إلتون جون...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مكنهدرش على باغي الفلوس على ود راحة البال اه...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>راه لي  مقتنع بلي هاد التضامن على  المواقع حام...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Commentaire   Classe  Label\n",
       "0   محند اعطيك الاختيارات ههههه و انت احسن الظن ف...   normal      2\n",
       "1     إبراهيم الجعفري رئيس وزراء العراق إلتون جون...  abusive      0\n",
       "2   لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...   normal      2\n",
       "3  تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...   normal      2\n",
       "4    مكنهدرش على باغي الفلوس على ود راحة البال اه...     hate      1\n",
       "5  راه لي  مقتنع بلي هاد التضامن على  المواقع حام...     hate      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "data_train['Label']=le.fit_transform(data_train['Classe'])\n",
    "data_train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa8bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e84394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "tokenizer=nltk.tokenize.WhitespaceTokenizer()\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "from snowballstemmer import stemmer\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "stopwords_list = stopwords.words('arabic')\n",
    "def preprocess(review):\n",
    "   review = re.sub('[^ا-ي]', ' ', review)\n",
    " \n",
    "   review = tokenizer.tokenize(review)\n",
    "   review = [ar_stemmer.stemWord(word) for word in review if not word in set(stopwords_list )]\n",
    "   review = ' '.join(review)\n",
    "   return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "595c6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=data_train['Label']\n",
    "X_train=data_train['Commentaire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e5a9003",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac75537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'لماذ عندم نتصفح ال نترن نشعر الكل عمر ويه صرح مونس يهم انا الله يعاون جميع صحاب بيولوج خصوص ههه كين عيش وحد هاد ذنيا او نادم عايش راك ودن ال كبير مثاب ال ال خت كبير مثاب ال مشكل تنس مشكل هاد عرب عفو ليه حسن ليك كيناضل كيناظل حق جميع هاد عيج منبعد اما باش تناضل غي مصلاح غي كتخربق كنت وحد بلاص مكيعرفوش كهربا تيليف عبر راس او نتاقد يعود لكرو عبر رواح ديال صيف صعييبب كلب فمو خانز ممنوع تصغير خوي حمانان ناس ناشط تغريدات علاال لوكيت واخ شنو ماش تخربيق قولي تفصيل شنو ندير باش منتيماش مخربق المل صداق كيخلع نادم هاد هدر فالمواقع كيحسابل راه بصح نقاش هه زعم راه دار انجاز ههه كيخلق بصح كين شبه اق اس اب ون يه كلااط درت ليه تهجيج انا شخصي ميمكنش ليا نتقد اسلام انا عمر طفرت فرض ديال ترب اسلام ههه خلي بلد نادم تكول ليه سلاام اجوب زون زون اكبر نصاب هاد بصح خوي دبدوب دك نت حب هاد ياك كاين تويتر لاخر معرفتوش مزيان حال والو نج شي يكاند نرجع اصدق لاتين قال ني اذا استمري صيام لشهر خر ختف راه غا نمش عيد ال غلب تيتبوش ليهم لافاب ديال دار تتضرب تلاف تتلق تيفت مشكل قنا سويس مكاينشا ريفورمد راك ماتحشمش الا ماهدي والو موروكا فلالس داو كيمرض مرجو دعم يحطول لة متفهمهومش حط ليهم جوب ميفهموهمش غا منين لقي مصو جاوب معم بسو الظن استحق جهد تفسير تعرض كار امتح كنت نتصن لراس مبر نجاح ههه فالش مكيكونش هاك حمد اسيد رب لحت مدين مفيهاش بحر شكون لمقدم الل ضبر واش اخوت مبارك كيداير كنتمني تكون حسن حالت هاد لحظ غابر دون سبب الله مستع صور لشبك طرق سيار روب لاحض مكان تمركز'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2] ### Après preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd4eda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' لماذا عندما نتصفح الإنترنت نشعر أن الكل من نفس عمرنا ويه صرحة مونس بيهم انا الله يعاون الجميع و صحاب البيولوجي بالخصوص ههه كين عيشة وحدة فهاد الذنيا او بنادم عايشها فبراكة و الودن الأخ الكبير بمثابة الأب و الأخت الكبيرة بمثابة الأم مشكلة تنسيك في مشكلة هاد العرب العفو غير رجع ليها حسن ليك لي كيناضل كيناظل على حق الجميع و حتى هادوك لي عيجيو منبعد اما باش تناضل غي على مصلاحتك غي كتخربق كنت فوحد البلاصة مكيعرفوش الكهرباء  و تيليفون عبر مع راسك او لي نتاقدك يعودها لكرو عبر الرواح ديال صيف صعييبب كلب فمو خانز ممنوع التصغير خويا حمانانت من ناس ناشطين مع تغريداتي علاال بلوكيتو واخا وشنو لي ماشي تخربيق قوليا بالتفصيل شنو ندير باش منتيماش مخربق المليون في صداق لي كيخلع هو بنادم هاد الهدرة فالمواقع ولا كيحسابليه راه بصح نقاش هه زعما راه دار انجاز ههه و كيخلق ض بصح كين الشبه اقْتَرَبَ لِلنَّاسِ حِسَابُهُمْ وَهُمْ فِي غَفْلَةٍ مُّعْرِضُونَ  مَا يَأْتِيهِم مِّن ذِكْرٍ مَّن رَّبِّهِم مُّ كلااطة درت ليه تهجيج انا شخصيا ميمكنش ليا نتقد الاسلام و انا عمرني طفرتو ف فرض ديال التربية اسلاميه ههههه أنا خليني غ مع البلدي بنادم تكول ليه السلاام اجوبك زون زون اكبر نصاب هادة بصحتك خويا دبدوبة دكية نتي حب هادة ياك واحد كاين هنا في تويتر لاخر معرفتوش مزيانة بحال والو نجي شي ويكاند و نرجع اصدقائي اللاتينيين قالوا لي بأني اذا ما استمريت في الصيام لشهر آخر سأختفي لا راه غا نمشي بعيد الأغلبية تيتبوشا ليهم لافابو ديال الدار تتضربهم تلافة وتتلقاهم حتى هما تيفتيو فمشكل قناة السويس مكاينشاي من الريفورمد  و راك ماتحشمش الا ماهديت والو للموروكا الفلالس بداو كيمرضو المرجو الدعم ب كي يحطولك أسئلة متفهمهومش حط ليهم أجوبة ميفهموهمش و غا منين لقيتني مصوني عليك جاوب المعمي بسوء الظن لا يستحق جهد التفسير تعرض للكار قبل الامتحان كنت نتصنت لراسي مبروك النجاح ههههههههههههه و فالشاء و مكيكونش هاكا الحمد اسيدي ربي لحتيني فمدينة مفيهاش بحر شكون لمقدم اللي ضبر عليك واش اخوتي جمعة مباركة كيدايرين كنتمنى تكونو حسن من حالتي فهاد اللحظة غابرين بدون سبب و الله المستعان صورة لشبكة الطرق السيارة في أوروبا لاحضو مكان تمركزها'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['Commentaire'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cb6a643e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19473"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the data into target and feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train ,test_size = 0.2, random_state=0)\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "vocab_size # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bbd4cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "tagged_docs_train=[gensim.models.doc2vec.TaggedDocument(word_list,[i]) for i,word_list in enumerate(X_train)]\n",
    "tagged_docs_test=[gensim.models.doc2vec.TaggedDocument(word_list,[i]) for i,word_list in enumerate(X_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "77c5a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model=gensim.models.Doc2Vec(tagged_docs_train, vector_size=160 ,window=5,min_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cbbe0bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [155]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_vectors\u001b[38;5;241m=\u001b[39m[d2v_model\u001b[38;5;241m.\u001b[39minfer_vector(i\u001b[38;5;241m.\u001b[39mwords)\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tagged_docs_train]\n\u001b[0;32m      2\u001b[0m test_vectors\u001b[38;5;241m=\u001b[39m[d2v_model\u001b[38;5;241m.\u001b[39minfer_vector(i\u001b[38;5;241m.\u001b[39mwords)\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tagged_docs_test]\n",
      "Input \u001b[1;32mIn [155]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_vectors\u001b[38;5;241m=\u001b[39m[\u001b[43md2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tagged_docs_train]\n\u001b[0;32m      2\u001b[0m test_vectors\u001b[38;5;241m=\u001b[39m[d2v_model\u001b[38;5;241m.\u001b[39minfer_vector(i\u001b[38;5;241m.\u001b[39mwords)\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tagged_docs_test]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\models\\doc2vec.py:628\u001b[0m, in \u001b[0;36mDoc2Vec.infer_vector\u001b[1;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[0;32m    625\u001b[0m min_alpha \u001b[38;5;241m=\u001b[39m min_alpha \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha\n\u001b[0;32m    626\u001b[0m epochs \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\n\u001b[1;32m--> 628\u001b[0m doctag_vectors \u001b[38;5;241m=\u001b[39m pseudorandom_weak_vector(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdv\u001b[38;5;241m.\u001b[39mvector_size, seed_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_words\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    629\u001b[0m doctag_vectors \u001b[38;5;241m=\u001b[39m doctag_vectors\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdv\u001b[38;5;241m.\u001b[39mvector_size)\n\u001b[0;32m    631\u001b[0m doctags_lockf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mREAL)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "train_vectors=[d2v_model.infer_vector(i.words)for i in tagged_docs_train]\n",
    "test_vectors=[d2v_model.infer_vector(i.words)for i in tagged_docs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7d1fc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from keras.layers import Embedding,Dense,GRU, Conv1D\n",
    "from keras.layers import Dropout,Bidirectional,GlobalMaxPool1D,GlobalAveragePooling1D\n",
    "from keras.layers import SpatialDropout1D,concatenate,Input\n",
    "# from keras.optimizers import RMSprop\n",
    "from keras.initializers import Constant\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f01ce484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "67db6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights=dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bca32f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محند اعطيك الاختيارات ههههه و انت احسن الظن ف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>إبراهيم الجعفري رئيس وزراء العراق إلتون جون...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مكنهدرش على باغي الفلوس على ود راحة البال اه...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>راه لي  مقتنع بلي هاد التضامن على  المواقع حام...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>بحرالرجز كاين مايتقال  خفت مايفهموش النقليزة ت...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Commentaire   Classe  Label\n",
       "0   محند اعطيك الاختيارات ههههه و انت احسن الظن ف...   normal      2\n",
       "1     إبراهيم الجعفري رئيس وزراء العراق إلتون جون...  abusive      0\n",
       "2   لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...   normal      2\n",
       "3  تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...   normal      2\n",
       "4    مكنهدرش على باغي الفلوس على ود راحة البال اه...     hate      1\n",
       "5  راه لي  مقتنع بلي هاد التضامن على  المواقع حام...     hate      1\n",
       "6  بحرالرجز كاين مايتقال  خفت مايفهموش النقليزة ت...   normal      2"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c8b7dc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len: 1339\n"
     ]
    }
   ],
   "source": [
    "lens_train = [len(i) for i in X_train]\n",
    "lens_test = [len(i) for i in X_test]\n",
    "lens = lens_train + lens_test\n",
    "\n",
    "maxlen = np.max(lens)\n",
    "print('Max len:', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80c9d405",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_matrix2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [169]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m embed_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Embedding Layer \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x\u001b[38;5;241m=\u001b[39mEmbedding(vocab_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,embed_size,embeddings_initializer\u001b[38;5;241m=\u001b[39mConstant(\u001b[43membedding_matrix2\u001b[49m),\n\u001b[0;32m      5\u001b[0m             input_length\u001b[38;5;241m=\u001b[39mmaxlen,trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(inp)\n\u001b[0;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m SpatialDropout1D(\u001b[38;5;241m0.2\u001b[39m)(x)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m### Couche CNN-BiGRU \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_matrix2' is not defined"
     ]
    }
   ],
   "source": [
    "inp=Input((maxlen,))\n",
    "embed_size = 300\n",
    "#Embedding Layer \n",
    "x=Embedding(vocab_size + 1,embed_size,embeddings_initializer=Constant(embedding_matrix2),\n",
    "            input_length=maxlen,trainable=True)(inp)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "### Couche CNN-BiGRU \n",
    "x=Conv1D(filters=32, kernel_size=3, padding='valid', activation='relu')(x) \n",
    "x=Bidirectional(GRU(150, return_sequences = True))(x)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPool1D()(x)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "\n",
    " \n",
    "x=Dropout(0.2)(conc)\n",
    "out=Dense(16, activation=\"relu\")(x)\n",
    "out=Dense(8, activation=\"relu\")(out)\n",
    "\n",
    "out=Dense(3, activation=\"softmax\")(out)\n",
    "\n",
    "model=Model(inp,out)\n",
    "\n",
    "mcp_save = ModelCheckpoint('model.mdl_wts.hdf5', save_best_only=True,\n",
    "                           monitor='val_loss', mode='min',patience = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8c8f1c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' محند اعطيك الاختيارات ههههه و انت احسن الظن فغشيطة بب ميس يو مؤخرا اكتشفت انني عايش ف اسهل بلد يمكن لك تجمع فيه النحاس و بديت نجمعو مانقدرش نقولكم سمو اول حاجة اجمعتها يمكن الله يرحمها او يبدل المحبة بالصبر مواليد   مارس دانيال بن سعيد فيلسوف فرنسي '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [179]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m model_2\u001b[38;5;241m=\u001b[39mDecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m (model_1,model_2):\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39m_class_\u001b[38;5;241m.\u001b[39m_name_, model\u001b[38;5;241m.\u001b[39mscore(X_test ,Y_test))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m _check_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_base.py:400\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 400\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:872\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;124;03m    Return the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m          dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ' محند اعطيك الاختيارات ههههه و انت احسن الظن فغشيطة بب ميس يو مؤخرا اكتشفت انني عايش ف اسهل بلد يمكن لك تجمع فيه النحاس و بديت نجمعو مانقدرش نقولكم سمو اول حاجة اجمعتها يمكن الله يرحمها او يبدل المحبة بالصبر مواليد   مارس دانيال بن سعيد فيلسوف فرنسي '"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "model_1 = KNeighborsClassifier(n_neighbors=3)\n",
    "model_2=DecisionTreeClassifier(random_state=0)\n",
    "for model in (model_1,model_2):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(model._class_._name_, model.score(X_test ,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b1748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
