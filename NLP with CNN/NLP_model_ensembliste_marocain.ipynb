{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68edc629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FIRAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FIRAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51308d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1549, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=pd.read_excel('C:/Users/FIRAS/Desktop/Ref NLP/work/dataset_marrocain.xlsx')\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92a62d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                             Commentaire   Classe\n",
       "0      محند اعطيك الاختيارات ههههه و انت احسن الظن ف...   normal\n",
       "1        إبراهيم الجعفري رئيس وزراء العراق إلتون جون...  abusive\n",
       "2      لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...   normal\n",
       "3     تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...   normal\n",
       "4       مكنهدرش على باغي الفلوس على ود راحة البال اه...     hate\n",
       "...                                                 ...      ...\n",
       "1544   يفكّرونَ رَبَّنَا الجنسيات والعربية للسعودين ...   normal\n",
       "1545                                     قل يامُسخّر لك   normal\n",
       "1546  تبغي تحماقغير الزمراوي فالاوزان المزيرية والقو...   normal\n",
       "1547  بقى برشيلونة بالقراءة المصنّفة بريطانيا الصهيو...   normal\n",
       "1548  لكن منتقلقوش كيتعرضوا والسجادة وتفرّقهم بروفاي...   normal\n",
       "\n",
       "[1549 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7eb4261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Commentaire', 'Classe'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b25d90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1548</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1485</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>هاد ناس برشيد عالام تحية للعنصرية النسوي</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12</td>\n",
       "      <td>1361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Commentaire  Classe\n",
       "count                                        1548    1549\n",
       "unique                                       1485       3\n",
       "top     هاد ناس برشيد عالام تحية للعنصرية النسوي   normal\n",
       "freq                                           12    1361"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b056f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Commentaire    1\n",
       "Classe         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f1bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad80ae7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Commentaire    False\n",
       "Classe         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bf9f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal     1360\n",
       "hate        166\n",
       "abusive      22\n",
       "Name: Classe, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['Classe'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "975f0156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Classe Distribution'}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train.Classe.value_counts().plot.bar(x=data_train.Classe.unique(), title='Classe Distribution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b7804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FIRAS\\AppData\\Local\\Temp\\ipykernel_6944\\1070480500.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['Label']=le.fit_transform(data_train['Classe'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محند اعطيك الاختيارات ههههه و انت احسن الظن ف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>إبراهيم الجعفري رئيس وزراء العراق إلتون جون...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مكنهدرش على باغي الفلوس على ود راحة البال اه...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>راه لي  مقتنع بلي هاد التضامن على  المواقع حام...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Commentaire   Classe  Label\n",
       "0   محند اعطيك الاختيارات ههههه و انت احسن الظن ف...   normal      2\n",
       "1     إبراهيم الجعفري رئيس وزراء العراق إلتون جون...  abusive      0\n",
       "2   لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...   normal      2\n",
       "3  تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...   normal      2\n",
       "4    مكنهدرش على باغي الفلوس على ود راحة البال اه...     hate      1\n",
       "5  راه لي  مقتنع بلي هاد التضامن على  المواقع حام...     hate      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "data_train['Label']=le.fit_transform(data_train['Classe'])\n",
    "data_train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f7aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FIRAS\\AppData\\Local\\Temp\\ipykernel_6944\\556673699.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['word_count'] = data_train['Commentaire'].apply(lambda x : len(x.split()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محند اعطيك الاختيارات ههههه و انت احسن الظن ف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>إبراهيم الجعفري رئيس وزراء العراق إلتون جون...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مكنهدرش على باغي الفلوس على ود راحة البال اه...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>يفكّرونَ رَبَّنَا الجنسيات والعربية للسعودين ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>قل يامُسخّر لك</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>تبغي تحماقغير الزمراوي فالاوزان المزيرية والقو...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>بقى برشيلونة بالقراءة المصنّفة بريطانيا الصهيو...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>لكن منتقلقوش كيتعرضوا والسجادة وتفرّقهم بروفاي...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1548 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Commentaire   Classe  Label  \\\n",
       "0      محند اعطيك الاختيارات ههههه و انت احسن الظن ف...   normal      2   \n",
       "1        إبراهيم الجعفري رئيس وزراء العراق إلتون جون...  abusive      0   \n",
       "2      لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...   normal      2   \n",
       "3     تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...   normal      2   \n",
       "4       مكنهدرش على باغي الفلوس على ود راحة البال اه...     hate      1   \n",
       "...                                                 ...      ...    ...   \n",
       "1544   يفكّرونَ رَبَّنَا الجنسيات والعربية للسعودين ...   normal      2   \n",
       "1545                                     قل يامُسخّر لك   normal      2   \n",
       "1546  تبغي تحماقغير الزمراوي فالاوزان المزيرية والقو...   normal      2   \n",
       "1547  بقى برشيلونة بالقراءة المصنّفة بريطانيا الصهيو...   normal      2   \n",
       "1548  لكن منتقلقوش كيتعرضوا والسجادة وتفرّقهم بروفاي...   normal      2   \n",
       "\n",
       "      word_count  \n",
       "0             47  \n",
       "1            117  \n",
       "2            345  \n",
       "3             17  \n",
       "4             44  \n",
       "...          ...  \n",
       "1544          31  \n",
       "1545           3  \n",
       "1546          58  \n",
       "1547          45  \n",
       "1548         153  \n",
       "\n",
       "[1548 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['word_count'] = data_train['Commentaire'].apply(lambda x : len(x.split()))\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c8abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pyarabic.araby as araby\n",
    "\n",
    "prefix_list = [\n",
    "    \"ال\",\n",
    "    \"و\",\n",
    "    \"ف\",\n",
    "    \"ب\",\n",
    "    \"ك\",\n",
    "    \"ل\",\n",
    "    \"لل\",\n",
    "    \"\\u0627\\u0644\",\n",
    "    \"\\u0648\",\n",
    "    \"\\u0641\",\n",
    "    \"\\u0628\",\n",
    "    \"\\u0643\",\n",
    "    \"\\u0644\",\n",
    "    \"\\u0644\\u0644\",\n",
    "    \"س\",\n",
    "]\n",
    "suffix_list = [\n",
    "    \"ه\",\n",
    "    \"ها\",\n",
    "    \"ك\",\n",
    "    \"ي\",\n",
    "    \"هما\",\n",
    "    \"كما\",\n",
    "    \"نا\",\n",
    "    \"كم\",\n",
    "    \"هم\",\n",
    "    \"هن\",\n",
    "    \"كن\",\n",
    "    \"ا\",\n",
    "    \"ان\",\n",
    "    \"ين\",\n",
    "    \"ون\",\n",
    "    \"وا\",\n",
    "    \"ات\",\n",
    "    \"ت\",\n",
    "    \"ن\",\n",
    "    \"ة\",\n",
    "    \"\\u0647\",\n",
    "    \"\\u0647\\u0627\",\n",
    "    \"\\u0643\",\n",
    "    \"\\u064a\",\n",
    "    \"\\u0647\\u0645\\u0627\",\n",
    "    \"\\u0643\\u0645\\u0627\",\n",
    "    \"\\u0646\\u0627\",\n",
    "    \"\\u0643\\u0645\",\n",
    "    \"\\u0647\\u0645\",\n",
    "    \"\\u0647\\u0646\",\n",
    "    \"\\u0643\\u0646\",\n",
    "    \"\\u0627\",\n",
    "    \"\\u0627\\u0646\",\n",
    "    \"\\u064a\\u0646\",\n",
    "    \"\\u0648\\u0646\",\n",
    "    \"\\u0648\\u0627\",\n",
    "    \"\\u0627\\u062a\",\n",
    "    \"\\u062a\",\n",
    "    \"\\u0646\",\n",
    "    \"\\u0629\",\n",
    "]\n",
    "other_tokens = [\"[رابط]\", \"[مستخدم]\", \"[بريد]\"]\n",
    "\n",
    "# the never_split list is ussed with the transformers library\n",
    "prefix_symbols = [x + \"+\" for x in prefix_list]\n",
    "suffix_symblos = [\"+\" + x for x in suffix_list]\n",
    "never_split_tokens = list(set(prefix_symbols + suffix_symblos + other_tokens))\n",
    "\n",
    "regex_url_step1 = r\"(?=http)[^\\s]+\"\n",
    "regex_url_step2 = r\"(?=www)[^\\s]+\"\n",
    "regex_url = r\"(http(s)?:\\/\\/.)?(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\n",
    "regex_mention = r\"@[\\w\\d]+\"\n",
    "regex_email = r\"\\S+@\\S+\"\n",
    "redundant_punct_pattern = (\n",
    "    r\"([!\\\"#\\$%\\'\\(\\)\\*\\+,\\.:;\\-<=·>?@\\[\\\\\\]\\^_ـ`{\\|}~—٪’،؟`୍“؛”ۚ【»؛\\s+«–…‘]{2,})\"\n",
    ")\n",
    "\n",
    "\n",
    "def remove_elongation(word):\n",
    "    \"\"\"\n",
    "\t:param word:  the input word to remove elongation\n",
    "\t:return: delongated word\n",
    "\t\"\"\"\n",
    "    regex_tatweel = r\"(\\w)\\1{2,}\"\n",
    "    # loop over the number of times the regex matched the word\n",
    "    for index_ in range(len(re.findall(regex_tatweel, word))):\n",
    "        if re.search(regex_tatweel, word):\n",
    "            elongation_found = re.search(regex_tatweel, word)\n",
    "            elongation_replacement = elongation_found.group()[0]\n",
    "            elongation_pattern = elongation_found.group()\n",
    "            word = re.sub(\n",
    "                elongation_pattern, elongation_replacement, word, flags=re.MULTILINE\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "    return word\n",
    "\n",
    "\n",
    "def tokenize_arabic_words_farasa(line_input, farasa_segmenter, use_farasapy):\n",
    "    if use_farasapy:\n",
    "        if type(farasa_segmenter).__name__ == \"FarasaSegmenter\":\n",
    "            line_farasa = farasa_segmenter.segment(line_input).split()\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                'use_farsapy is set to True. farasa must be a \"py4j.java_gateway.JavaObject\"'\n",
    "            )\n",
    "    else:\n",
    "        if type(farasa_segmenter).__name__ == \"JavaObject\":\n",
    "            line_farasa = farasa_segmenter.segmentLine(line_input)\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                'use_farsapy is set to False. farasa must be a \"FarasaSegmenter\" instance from farasapy.segmenter'\n",
    "            )\n",
    "    segmented_line = []\n",
    "    for index, word in enumerate(line_farasa):\n",
    "        if word in [\"[\", \"]\"]:\n",
    "            continue\n",
    "        if word in [\"رابط\", \"بريد\", \"مستخدم\"] and line_farasa[index - 1] in [\"[\", \"]\"]:\n",
    "            segmented_line.append(\"[\" + word + \"]\")\n",
    "            continue\n",
    "        segmented_word = []\n",
    "        for token in word.split(\"+\"):\n",
    "            if token in prefix_list:\n",
    "                segmented_word.append(token + \"+\")\n",
    "            elif token in suffix_list:\n",
    "                segmented_word.append(\"+\" + token)\n",
    "            else:\n",
    "                segmented_word.append(token)\n",
    "        segmented_line.extend(segmented_word)\n",
    "    return \" \".join(segmented_line)\n",
    "\n",
    "\n",
    "def remove_redundant_punct(text):\n",
    "    text_ = text\n",
    "    result = re.search(redundant_punct_pattern, text)\n",
    "    dif = 0\n",
    "    while result:\n",
    "        sub = result.group()\n",
    "        sub = sorted(set(sub), key=sub.index)\n",
    "        sub = \" \" + \"\".join(list(sub)) + \" \"\n",
    "        text = \"\".join(\n",
    "            (text[: result.span()[0] + dif], sub, text[result.span()[1] + dif :])\n",
    "        )\n",
    "        text_ = \"\".join((text_[: result.span()[0]], text_[result.span()[1] :])).strip()\n",
    "        dif = abs(len(text) - len(text_))\n",
    "        result = re.search(redundant_punct_pattern, text_)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess(text, do_farasa_tokenization=True, farasa=None, use_farasapy=False):\n",
    "    \"\"\"\n",
    "\tPreprocess takes an input text line an applies the same preprocessing used in araBERT \n",
    "\t\t\t\tpretraining\n",
    "\t\n",
    "\tNote: a farasapy segmenter is ~6x faster than the py4j.java_gateway, consider setting use_farasapy=True\n",
    "\tFarsa Segmentation will soon be fully migrated to farasapy, and support for the py4j.java_gateway.JavaObject will be removed\n",
    "\tArgs:\n",
    "\t\ttext (string): inout text string\n",
    "\t\tfarasa (JavaGateway): pass a \"py4j.java_gateway.JavaObject\" to the FarasaSegmenter.jar file \n",
    "\t\tfarasa (FarasaSegmenter): pass a FarasaSegmenter instance from farasapy.segmenter\n",
    "\t\tuse_farasapy (boolean): set it to True when using a FarasaSegmenter instance from farasapy.segmenter\n",
    "\tExample: \n",
    "\t\tfrom py4j.java_gateway import JavaGateway\n",
    "\t\tgateway = JavaGateway.launch_gateway(classpath='./FarasaSegmenterJar.jar')\n",
    "\t\tfarasa = gateway.jvm.com.qcri.farasa.segmenter.Farasa()\n",
    "\t\tprocessed_text = preprocess(\"Some_Text\",do_farasa_tokenization=True , farasa=farasa)\n",
    "\t\"\"\"\n",
    "    text = str(text)\n",
    "    processing_tweet = araby.strip_tashkeel(text)\n",
    "    processing_tweet = re.sub(r\"\\d+\\/[ء-ي]+\\/\\d+\\]\", \"\", processing_tweet)\n",
    "    processing_tweet = re.sub(\"ـ\", \"\", processing_tweet)\n",
    "    processing_tweet = re.sub(\"[«»]\", ' \" ', processing_tweet)\n",
    "    # replace the [رابط] token with space if you want to clean links\n",
    "    processing_tweet = re.sub(regex_url_step1, \"[رابط]\", processing_tweet)\n",
    "    processing_tweet = re.sub(regex_url_step2, \"[رابط]\", processing_tweet)\n",
    "    processing_tweet = re.sub(regex_url, \"[رابط]\", processing_tweet)\n",
    "    processing_tweet = re.sub(regex_email, \"[بريد]\", processing_tweet)\n",
    "    processing_tweet = re.sub(regex_mention, \"[مستخدم]\", processing_tweet)\n",
    "    processing_tweet = re.sub(\"…\", r\"\\.\", processing_tweet).strip()\n",
    "    processing_tweet = remove_redundant_punct(processing_tweet)\n",
    "\n",
    "    processing_tweet = re.sub(\n",
    "        r\"\\[ رابط \\]|\\[ رابط\\]|\\[رابط \\]\", \" [رابط] \", processing_tweet\n",
    "    )\n",
    "    processing_tweet = re.sub(\n",
    "        r\"\\[ بريد \\]|\\[ بريد\\]|\\[بريد \\]\", \" [بريد] \", processing_tweet\n",
    "    )\n",
    "    processing_tweet = re.sub(\n",
    "        r\"\\[ مستخدم \\]|\\[ مستخدم\\]|\\[مستخدم \\]\", \" [مستخدم] \", processing_tweet\n",
    "    )\n",
    "\n",
    "    processing_tweet = remove_elongation(processing_tweet)\n",
    "    if do_farasa_tokenization and farasa is not None:\n",
    "        processing_tweet = tokenize_arabic_words_farasa(\n",
    "            processing_tweet, farasa, use_farasapy\n",
    "        )\n",
    "    return processing_tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8e5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=data_train['Label']\n",
    "X_train=data_train['Commentaire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e214c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f07678a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'راه لي مقتنع بلي هاد التضامن على المواقع حامض و معندو باش يفيد و بعقلو راه گاع ممسوق لهاد الحركات داخ شكاتسنا من واحد الطموح ديالو منحصر فانه يكون عندو كدوي مع راجل فالمسائل اليدنية كيبقى يجبدلك الحجاب و الارث و الزواج ه بحالي هو لي عيكوز نهار دخلة هه دوي بالعلم ديور كيرتاحوا فيهم بنات الناس دار واليديهم او دار رجالهم لا عادي ه غي انت كدري متقدش تخيل راسك بنت كل و خليقتو ترجلوا شوية الى تبعنى الزين انصدقوا فطوانغ الا دق عليك شي واحد غي حاول تجنبو حيت هو جاي بدك الفكرة ديال غي باغي يطلعها وانت اتحاوب منيتك وفهاد الحالة ديما اتخسر لي مريض بينا الله يشافيه عيد مبارك سعيد أدخله الله عليك بالنجاح والتوفيق في مسارك الدراسي و وبشي حجة للوالدين إن شاء الله ️ إذ لم توفر خبزا للفقراء لن يتوفر الأمن للاغنياء تمشي درب دويرة فغزة وتجي برب تا تكتب على القضية الفليسطسنة فلحمك بمسمار ماشي غي فتويتر واش تويتر بلاصة يسول فيها الواحد وش حسابليك مفلويك الشيخ الازهر باش يبقى يجاوبك را الاغلبية هنا ميكون غي الخير عندك عقل خصك غ الزين تلفتونا الله تلفها عليكم احكومة الحبس منطقة واحد منطقة جوج ممنوع تجمعات ممنوع تصاحب الموت اخويا الموت اياك نعبد واياك نستعين بغيت نشوف غي كيصبح الجو ولكن عادل المنكوليان من نهار بديت كنتمزك لمورفين مبقاتش كتشدني لخلعة ديال الامتحانات مور رمضان معرفتش لمهم الناس ولات عدائية كثر مما كانت قبل رمضان كازة جريت فيها قبل اتزاد غنجيبو نبقي نعصر عليه غ الليمون م عنديش مع رقابة عاشت الحرية عطيونا غ حقنا من البيرات ولوحنا لشي زون ديال عكاشة كع م تجيبوش معانا غ القرطبي نتي هادي ربي معاك يله الله فيقنا بعيبنا قبل مي فيقو بيه الناس كولش تروج اخبار عن اعتقال يوسف الزروالي بتهمة النصب والتشهير عندكم شي خبار البريمة من نيتك الخراي ديال المولفري غنموتو ونحياو وغيبقي هو داوي غ علي جنس يا لطيف فين عمرك شفتيني الكدابة ليست المدينة نفسها و لكن الحفر تتشابه'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5] ### Après preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "433d4367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'راه لي  مقتنع بلي هاد التضامن على  المواقع حامض و معندو باش يفيد و بعقلو راه گاع ممسوق لهاد الحركات داخ شكاتسنا من واحد الطموح ديالو منحصر فانه يكون عندو كدوي مع راجل فالمسائل اليدنية كيبقى يجبدلك الحجاب و الارث و الزواج ههه بحالي هو لي عيكوز نهار دخلة هههه دوي بالعلم ديور كيرتاحوا فيهم بنات الناس دار واليديهم او دار رجالهم لا عادي ههه غي انت كدري متقدش تخيل راسك بنت كل و خليقتو ترجلوا شوية الى تبعنى الزين انصدقوا فطوانغ الا دق عليك شي واحد غي حاول تجنبو حيت هو جاي بدك الفكرة ديال غي باغي يطلعها وانت اتحاوب منيتك وفهاد الحالة ديما اتخسر لي مريض بينا الله يشافيه عيد مبارك سعيد أدخله الله عليك بالنجاح والتوفيق في مسارك الدراسي و وبشي حجة للوالدين إن شاء الله ️ إذ لم توفر خبزا للفقراء لن يتوفر الأمن للاغنياء تمشي درب دويرة فغزة وتجي برب تا تكتب على القضية الفليسطسنة فلحمك بمسمار ماشي غي فتويتر واش تويتر بلاصة يسول فيها الواحد وش حسابليك مفلويك الشيخ الازهر باش يبقى يجاوبك را الاغلبية هنا ميكون غي الخير عندك عقل خصك غ الزين تلفتونا الله تلفها عليكم احكومة الحبس منطقة واحد منطقة جوج ممنوع تجمعات ممنوع تصاحب الموت اخويا الموت اياك نعبد واياك نستعين بغيت نشوف غي كيصبح الجو ولكن عادل المنكوليان من نهار بديت كنتمزك لمورفين مبقاتش كتشدني لخلعة ديال الامتحانات مور رمضان معرفتش لمهم الناس ولات عدائية كثر مما كانت قبل رمضان كازة جريت فيها قبل اتزاد غنجيبو نبقي نعصر عليه غ الليمون م عنديش مع رقابة عاشت الحرية عطيونا غ حقنا من البيرات ولوحنا لشي زون ديال عكاشة كع م تجيبوش معانا غ القرطبي نتي هادي ربي معاك يله الله فيقنا بعيبنا قبل مي فيقو بيه الناس كولش تروج اخبار عن اعتقال يوسف الزروالي  بتهمة النصب والتشهير عندكم شي خبار البريمة من نيتك الخراي ديال المولفري غنموتو ونحياو وغيبقي هو داوي غ علي جنس يا لطيف فين عمرك شفتيني الكدابة ليست المدينة نفسها  و لكن الحفر تتشابه'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['Commentaire'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03501090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, BertModel, Trainer, TrainingArguments, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b1186ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1083, 232, 233)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the data into target and feature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(data_train['Commentaire'], data_train['Classe'], random_state=42, \n",
    "                                                                    test_size=0.3)\n",
    "\n",
    "train_texts=train_texts.apply(preprocess)\n",
    "temp_texts=temp_texts.apply(preprocess)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, random_state=42, \n",
    "                                                                test_size=0.5)\n",
    "\n",
    "len(train_texts), len(val_texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1662d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'alger-ia/dziribert'\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e2c5298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAW60lEQVR4nO3df2xd5X3H8fdnpNAW0zhAdxU52ZKOrBUjKiQWBPWHbLJSknZNtrWIKhqBRfIm0V+j1UhXae2kTQsrKSq0ovMa1qRKMZQWJaK0NAu4FX8kbQJpws/GpElJFuxBflBD2i7dd3/cJ3BtbN/r63uvfZ98XtLVfc5znnPu9xwnHx8/91xbEYGZmeXl9ya7ADMzqz2Hu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhioKd0l/J+kJSY9LukvSGyXNlbRdUp+kuyWdmcaelZb70vo5dT0CMzN7nbLhLqkN+ATQHhEXAWcA1wA3A7dGxAXAUWBV2mQVcDT135rGmZlZA1U6LTMNeJOkacCbgcPAFcC9af16YHlqL0vLpPWLJakm1ZqZWUWmlRsQEYck3QL8EjgB/BDYCRyLiJNp2EGgLbXbgOfSticlHQfOA14Y7TXOP//8mDNnTlUH8PLLL3P22WdXtW2judb6cK3100z1no617ty584WIeOtI68qGu6QZFK/G5wLHgG8DV020KEldQBdAoVDglltuqWo/g4ODtLS0TLSchnCt9eFa66eZ6j0da+3s7Dww6sqIGPMBfARYV7J8LXAHxSvxaanvcuDB1H4QuDy1p6VxGus1Fi5cGNV6+OGHq9620VxrfbjW+mmmek/HWoEdMUquVjLn/ktgkaQ3p7nzxcCTwMPAh9OYlcCm1N6clknrH0pFmJlZg5QN94jYTvGN0UeBPWmbbuAm4EZJfRTn1NelTdYB56X+G4HVdajbzMzGUHbOHSAiPg98flj3PuDSEcb+muJUjpmZTRJ/QtXMLEMOdzOzDDnczcwy5HA3M8uQw93MLEMV3S0zle05dJzrVn+v7Lj9az7QgGrMzKYGX7mbmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZKhvukt4uaVfJ4yVJn5J0rqQtkvam5xlpvCTdJqlP0m5JC+p/GGZmVqqSP5D9TERcHBEXAwuBV4D7KP7h660RMQ/Yymt/CHsJMC89uoA76lC3mZmNYbzTMouBZyPiALAMWJ/61wPLU3sZsCGKtgGtkmbWolgzM6vMeMP9GuCu1C5ExOHUfh4opHYb8FzJNgdTn5mZNYgiorKB0pnAfwN/EhH9ko5FRGvJ+qMRMUPS/cCaiHgk9W8FboqIHcP210Vx2oZCobCwp6enqgMYOHKc/hPlx81vm17V/mtpcHCQlpaWyS6jIq61PpqpVmiuek/HWjs7O3dGRPtI68bzl5iWAI9GRH9a7pc0MyIOp2mXgdR/CJhdst2s1DdERHQD3QDt7e3R0dExjlJec/vGTazdU/4w9q+obv+11NvbS7XH2WiutT6aqVZornpd61DjmZb5KK9NyQBsBlam9kpgU0n/temumUXA8ZLpGzMza4CKrtwlnQ28D/ibku41wD2SVgEHgKtT/wPAUqCP4p0119esWjMzq0hF4R4RLwPnDet7keLdM8PHBnBDTaozM7Oq+BOqZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWWoonCX1CrpXklPS3pK0uWSzpW0RdLe9DwjjZWk2yT1SdotaUF9D8HMzIar9Mr9y8APIuIdwDuBp4DVwNaImAdsTcsAS4B56dEF3FHTis3MrKyy4S5pOvBeYB1ARPw2Io4By4D1adh6YHlqLwM2RNE2oFXSzBrXbWZmY6jkyn0u8D/Af0p6TNLXJZ0NFCLicBrzPFBI7TbguZLtD6Y+MzNrEEXE2AOkdmAb8K6I2C7py8BLwMcjorVk3NGImCHpfmBNRDyS+rcCN0XEjmH77aI4bUOhUFjY09NT1QEMHDlO/4ny4+a3Ta9q/7U0ODhIS0vLZJdREddaH81UKzRXvadjrZ2dnTsjon2kddMq2P4gcDAitqfleynOr/dLmhkRh9O0y0BafwiYXbL9rNQ3RER0A90A7e3t0dHRUcmxvM7tGzexdk/5w9i/orr911Jvby/VHmejudb6aKZaobnqda1DlZ2WiYjngeckvT11LQaeBDYDK1PfSmBTam8Grk13zSwCjpdM35iZWQNUcuUO8HFgo6QzgX3A9RS/MdwjaRVwALg6jX0AWAr0Aa+ksWZm1kAVhXtE7AJGmtdZPMLYAG6YWFlmZjYR/oSqmVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZaiicJe0X9IeSbsk7Uh950raImlvep6R+iXpNkl9knZLWlDPAzAzs9cbz5V7Z0RcHBGn/lD2amBrRMwDtqZlgCXAvPToAu6oVbFmZlaZiUzLLAPWp/Z6YHlJ/4Yo2ga0Spo5gdcxM7NxUkSUHyT9AjgKBPDvEdEt6VhEtKb1Ao5GRKuk+4E1EfFIWrcVuCkidgzbZxfFK3sKhcLCnp6eqg5g4Mhx+k+UHze/bXpV+6+lwcFBWlpaJruMirjW+mimWqG56j0da+3s7NxZMpsyxLQK9/HuiDgk6feBLZKeLl0ZESGp/HeJodt0A90A7e3t0dHRMZ7NX3X7xk2s3VP+MPavqG7/tdTb20u1x9lorrU+mqlWaK56XetQFU3LRMSh9DwA3AdcCvSfmm5JzwNp+CFgdsnms1KfmZk1SNlwl3S2pHNOtYErgceBzcDKNGwlsCm1NwPXprtmFgHHI+JwzSs3M7NRVTItUwDuK06rMw34VkT8QNJPgXskrQIOAFen8Q8AS4E+4BXg+ppXbWZmYyob7hGxD3jnCP0vAotH6A/ghppUZ2ZmVfEnVM3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLUMXhLukMSY9Juj8tz5W0XVKfpLslnZn6z0rLfWn9nDrVbmZmoxjPlfsngadKlm8Gbo2IC4CjwKrUvwo4mvpvTePMzKyBKgp3SbOADwBfT8sCrgDuTUPWA8tTe1laJq1fnMabmVmDKCLKD5LuBf4VOAf4DHAdsC1dnSNpNvD9iLhI0uPAVRFxMK17FrgsIl4Yts8uoAugUCgs7OnpqeoABo4cp/9E+XHz26ZXtf9aGhwcpKWlZbLLqIhrrY9mqhWaq97TsdbOzs6dEdE+0rpp5TaW9EFgICJ2SuqYcDVJRHQD3QDt7e3R0VHdrm/fuIm1e8oeBvtXVLf/Wurt7aXa42w011ofzVQrNFe9rnWo8qkI7wI+JGkp8EbgLcCXgVZJ0yLiJDALOJTGHwJmAwclTQOmAy/WvHIzMxtV2Tn3iPhsRMyKiDnANcBDEbECeBj4cBq2EtiU2pvTMmn9Q1HJ3I+ZmdXMRO5zvwm4UVIfcB6wLvWvA85L/TcCqydWopmZjVcl0zKvioheoDe19wGXjjDm18BHalCbmZlVyZ9QNTPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDZcNd0hsl/UTSzyQ9IemfUv9cSdsl9Um6W9KZqf+stNyX1s+p8zGYmdkwlVy5/wa4IiLeCVwMXCVpEXAzcGtEXAAcBVal8auAo6n/1jTOzMwaqGy4R9FgWnxDegRwBXBv6l8PLE/tZWmZtH6xJNWqYDMzK08RUX6QdAawE7gA+CrwRWBbujpH0mzg+xFxkaTHgasi4mBa9yxwWUS8MGyfXUAXQKFQWNjT01PVAQwcOU7/ifLj5rdNr2r/tTQ4OEhLS8tkl1ER11ofzVQrNFe9p2OtnZ2dOyOifaR10yrZQUT8DrhYUitwH/COiRYVEd1AN0B7e3t0dHRUtZ/bN25i7Z7yh7F/RXX7r6Xe3l6qPc5Gc6310Uy1QnPV61qHGtfdMhFxDHgYuBxolXQqVWcBh1L7EDAbIK2fDrxYi2LNzKwyldwt89Z0xY6kNwHvA56iGPIfTsNWAptSe3NaJq1/KCqZ+zEzs5qpZFpmJrA+zbv/HnBPRNwv6UmgR9I/A48B69L4dcA3JfUBR4Br6lC3mZmNoWy4R8Ru4JIR+vcBl47Q/2vgIzWpzszMquJPqJqZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWoUr+QPZsSQ9LelLSE5I+mfrPlbRF0t70PCP1S9Jtkvok7Za0oN4HYWZmQ1Vy5X4S+HREXAgsAm6QdCGwGtgaEfOArWkZYAkwLz26gDtqXrWZmY2pbLhHxOGIeDS1fwU8BbQBy4D1adh6YHlqLwM2RNE2oFXSzFoXbmZmoxvXnLukOcAlwHagEBGH06rngUJqtwHPlWx2MPWZmVmDKCIqGyi1AD8C/iUivivpWES0lqw/GhEzJN0PrImIR1L/VuCmiNgxbH9dFKdtKBQKC3t6eqo6gIEjx+k/UX7c/LbpVe2/lgYHB2lpaZnsMiriWuujmWqF5qr3dKy1s7NzZ0S0j7RuWiU7kPQG4DvAxoj4burulzQzIg6naZeB1H8ImF2y+azUN0REdAPdAO3t7dHR0VFJKa9z+8ZNrN1T/jD2r6hu/7XU29tLtcfZaK61PpqpVmiuel3rUJXcLSNgHfBURHypZNVmYGVqrwQ2lfRfm+6aWQQcL5m+MTOzBqjkyv1dwF8BeyTtSn3/AKwB7pG0CjgAXJ3WPQAsBfqAV4Dra1mwmZmVVzbc09y5Rlm9eITxAdwwwbrMzGwC/AlVM7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDJUyR/IvlPSgKTHS/rOlbRF0t70PCP1S9Jtkvok7Za0oJ7Fm5nZyCr5A9nfAL4CbCjpWw1sjYg1klan5ZuAJcC89LgMuCM9T7o5q79X8dj9az5Qx0rMzOqv7JV7RPwYODKsexmwPrXXA8tL+jdE0TagVdLMGtVqZmYVqnbOvRARh1P7eaCQ2m3AcyXjDqY+MzNrIEVE+UHSHOD+iLgoLR+LiNaS9UcjYoak+4E1EfFI6t8K3BQRO0bYZxfQBVAoFBb29PRUdQADR47Tf6KqTUc1v216bXeYDA4O0tLSUpd915prrY9mqhWaq97TsdbOzs6dEdE+0rpK5txH0i9pZkQcTtMuA6n/EDC7ZNys1Pc6EdENdAO0t7dHR0dHVYXcvnETa/dUexgj27+iulrK6e3tpdrjbDTXWh/NVCs0V72udahqp2U2AytTeyWwqaT/2nTXzCLgeMn0jZmZNUjZS15JdwEdwPmSDgKfB9YA90haBRwArk7DHwCWAn3AK8D1dajZzMzKKBvuEfHRUVYtHmFsADdMtCgzM5uY2k5WZ6LSe+J9P7yZTVX+9QNmZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmG/CGmCfCHncxsqvKVu5lZhhzuZmYZcribmWXI4W5mliG/odoAp954/fT8k1w3xpuwfuPVzGrFV+5mZhlyuJuZZcjTMlOI75s3s1qpy5W7pKskPSOpT9LqeryGmZmNruZX7pLOAL4KvA84CPxU0uaIeLLWr3W68hW+mZVTj2mZS4G+iNgHIKkHWAY43Bus0m8CIxntzh5/wzBrDvUI9zbguZLlg8BldXgdmwS1/qnBP4WY1cekvaEqqQvoSouDkp6pclfnAy/Upqr6+sRpVKturmEx5ffXNOeV5qoVmqve07HWPxxtRT3C/RAwu2R5VuobIiK6ge6JvpikHRHRPtH9NIJrrQ/XWj/NVK9rHaoed8v8FJgnaa6kM4FrgM11eB0zMxtFza/cI+KkpI8BDwJnAHdGxBO1fh0zMxtdXebcI+IB4IF67HsEE57aaSDXWh+utX6aqV7XWkIRUe/XMDOzBvPvljEzy1DThvtU+xUHkmZLeljSk5KekPTJ1P8FSYck7UqPpSXbfDbV/4yk9ze43v2S9qSadqS+cyVtkbQ3Pc9I/ZJ0W6p1t6QFDa717SXnb5eklyR9aqqcW0l3ShqQ9HhJ37jPpaSVafxeSSsbWOsXJT2d6rlPUmvqnyPpRMn5/VrJNgvTv5++dDxqUK3j/po3IitGqfXukjr3S9qV+htzXiOi6R4U36h9FngbcCbwM+DCSa5pJrAgtc8Bfg5cCHwB+MwI4y9MdZ8FzE3Hc0YD690PnD+s79+A1am9Grg5tZcC3wcELAK2T/LX/nmK9/dOiXMLvBdYADxe7bkEzgX2pecZqT2jQbVeCUxL7ZtLap1TOm7Yfn6S6lc6niUNqnVcX/NGZcVItQ5bvxb4x0ae12a9cn/1VxxExG+BU7/iYNJExOGIeDS1fwU8RfHTuqNZBvRExG8i4hdAH8XjmkzLgPWpvR5YXtK/IYq2Aa2SZk5CfQCLgWcj4sAYYxp6biPix8CREWoYz7l8P7AlIo5ExFFgC3BVI2qNiB9GxMm0uI3iZ1NGlep9S0Rsi2IibeC146trrWMY7WvekKwYq9Z09X01cNdY+6j1eW3WcB/pVxyMFaQNJWkOcAmwPXV9LP3Ie+epH8+Z/GMI4IeSdqr4aWGAQkQcTu3ngUJqT3atpa5h6H+SqXhuYfzncirUDPDXFK8YT5kr6TFJP5L0ntTXRrG+Uxpd63i+5lPhvL4H6I+IvSV9dT+vzRruU5akFuA7wKci4iXgDuCPgIuBwxR/PJsK3h0RC4AlwA2S3lu6Ml05TKlbqVT8UNyHgG+nrql6boeYiudyJJI+B5wENqauw8AfRMQlwI3AtyS9ZbLqS5riaz7MRxl6QdKQ89qs4V7RrzhoNElvoBjsGyPiuwAR0R8Rv4uI/wP+g9emByb1GCLiUHoeAO5LdfWfmm5JzwNTodYSS4BHI6Ifpu65TcZ7Lie1ZknXAR8EVqRvRqQpjhdTeyfFues/TnWVTt00rNYqvuaTfV6nAX8B3H2qr1HntVnDfcr9ioM0r7YOeCoivlTSXzo3/efAqXfTNwPXSDpL0lxgHsU3UxpR69mSzjnVpviG2uOpplN3aawENpXUem2602MRcLxkyqGRhlwBTcVzW2K85/JB4EpJM9JUw5Wpr+4kXQX8PfChiHilpP+tKv59BiS9jeJ53JfqfUnSovTv/tqS46t3reP9mk92Vvwp8HREvDrd0rDzWut3jRv1oHjXwc8pftf73BSo590Uf/TeDexKj6XAN4E9qX8zMLNkm8+l+p+hDncbjFHr2yjeNfAz4IlT5w84D9gK7AX+Czg39YviH2B5Nh1L+ySc37OBF4HpJX1T4txS/IZzGPhfivOkq6o5lxTnu/vS4/oG1tpHcV761L/br6Wxf5n+fewCHgX+rGQ/7RSD9VngK6QPRDag1nF/zRuRFSPVmvq/AfztsLENOa/+hKqZWYaadVrGzMzG4HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDP0/sb8FInLyqYYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(tokenizer.encode(i)) for i in train_texts]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cd2bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 40\n",
    "\n",
    "train_encodings = tokenizer(train_texts.to_list(), truncation=True, padding=True, return_token_type_ids=False, max_length=max_seq_len)\n",
    "val_encodings = tokenizer(val_texts.to_list(), truncation=True, padding=True, return_token_type_ids=False, max_length=max_seq_len)\n",
    "test_encodings = tokenizer(test_texts.to_list(), truncation=True, padding=True, return_token_type_ids=False, max_length=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c4eecd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4114022534.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [23]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"Transform xml file to pandas series core...\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, encodings, labels=None):\n",
    "\t\tself.encodings = encodings\n",
    "\t\tself.labels = labels\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\titem = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\t\tif self.labels:\n",
    "\t\t\titem[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "\t\t\t#print(item)\n",
    "\t\treturn item\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\tprint(len(self.encodings[\"input_ids\"]))\n",
    "\t\treturn len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "# prepare dat for classification\n",
    "\n",
    "tokenizer = XXXTokenizer.from_pretrained(model_name)\n",
    "\tprint(\"Transform xml file to pandas series core...\")\n",
    "\ttext, file_name = transform_xml_to_pd(file)  # transform xml file to pd\n",
    "\t\n",
    "\t# Xtest_emb, s = get_xxx_layer(Xtest['sent'], path_to_model_lge)  # index 2 correspond to sentences\n",
    "\t#print(text)\n",
    "\t\n",
    "\tprint(\"Preprocess text with spacy model...\")\n",
    "\tclean_text = make_new_traindata(text['sent'])\n",
    "\t#print(clean_text[1])  # clean text ; 0 = raw text ; and etc...\n",
    "\t\n",
    "\tX = list(clean_text)\n",
    "\tX_text_tokenized = []\n",
    "\t\n",
    "\tfor x in X:\n",
    "\t\t#print(type(x))\n",
    "\t\tx_encoded = tokenizer(str(x), padding=\"max_length\", truncation=True, max_length=512)\n",
    "\t\t#print(type(x_encoded))\n",
    "\t\t#print(x_encoded)\n",
    "\t\tX_text_tokenized.append(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfc7a1cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_text_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     values \u001b[38;5;241m=\u001b[39m [dic\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mfor\u001b[39;00m dic \u001b[38;5;129;01min\u001b[39;00m d]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mlist\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mvalues))}\n\u001b[1;32m----> 7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m Dataset(list_of_dicts_to_dict_of_lists(\u001b[43mX_text_tokenized\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_text_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "def list_of_dicts_to_dict_of_lists(d):\n",
    "    dic = d[0]\n",
    "    keys = dic.keys()\n",
    "    values = [dic.values() for dic in d]\n",
    "    return {k: list(v) for k, v in zip(keys, zip(*values))}\n",
    "\n",
    "train_dataset = Dataset(list_of_dicts_to_dict_of_lists(X_text_tokenized))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcf9195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = preds.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "518f1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e8d01f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\FIRAS/.cache\\huggingface\\transformers\\c179cf24640b1b0dda4446337f9b96b5c03874251d8f55598a0c05fb34f62a25.9fcdc9919e76b1cfb4bd7813db6afefee31f549a2de09ab0105e38953d8d7781' at 'C:\\Users\\FIRAS/.cache\\huggingface\\transformers\\c179cf24640b1b0dda4446337f9b96b5c03874251d8f55598a0c05fb34f62a25.9fcdc9919e76b1cfb4bd7813db6afefee31f549a2de09ab0105e38953d8d7781'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:461\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:705\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    704\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m--> 705\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:243\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[1;34m(self, name_or_buffer)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_reader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:465\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou cloned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1971: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBERT_MODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2132\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n\u001b[0;32m   2130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2131\u001b[0m         \u001b[38;5;66;03m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2132\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2134\u001b[0m     \u001b[38;5;66;03m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2135\u001b[0m     \u001b[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m     \u001b[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2137\u001b[0m     \u001b[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2138\u001b[0m     \u001b[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2139\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:477\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    473\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    475\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\FIRAS/.cache\\huggingface\\transformers\\c179cf24640b1b0dda4446337f9b96b5c03874251d8f55598a0c05fb34f62a25.9fcdc9919e76b1cfb4bd7813db6afefee31f549a2de09ab0105e38953d8d7781' at 'C:\\Users\\FIRAS/.cache\\huggingface\\transformers\\c179cf24640b1b0dda4446337f9b96b5c03874251d8f55598a0c05fb34f62a25.9fcdc9919e76b1cfb4bd7813db6afefee31f549a2de09ab0105e38953d8d7781'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "model_1 = BertModel.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81875c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Commentaire</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Label</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محند اعطيك الاختيارات ههههه و انت احسن الظن ف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>إبراهيم الجعفري رئيس وزراء العراق إلتون جون...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مكنهدرش على باغي الفلوس على ود راحة البال اه...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>راه لي  مقتنع بلي هاد التضامن على  المواقع حام...</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Commentaire   Classe  Label  \\\n",
       "0   محند اعطيك الاختيارات ههههه و انت احسن الظن ف...   normal      2   \n",
       "1     إبراهيم الجعفري رئيس وزراء العراق إلتون جون...  abusive      0   \n",
       "2   لماذا عندما نتصفح الإنترنت نشعر أن الكل من نف...   normal      2   \n",
       "3  تريد دابا كنقادو بروغرام ديال الصيف ههه بنادم ...   normal      2   \n",
       "4    مكنهدرش على باغي الفلوس على ود راحة البال اه...     hate      1   \n",
       "5  راه لي  مقتنع بلي هاد التضامن على  المواقع حام...     hate      1   \n",
       "\n",
       "   word_count  \n",
       "0          47  \n",
       "1         117  \n",
       "2         345  \n",
       "3          17  \n",
       "4          44  \n",
       "5         317  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d01f3c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'farasa_segmenter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m data_train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [DATA_COLUMN, LABEL_COLUMN]\n\u001b[0;32m      7\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m }\n\u001b[1;32m---> 12\u001b[0m data_train[DATA_COLUMN] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDATA_COLUMN\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_farasa_tokenization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfarasa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfarasa_segmenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_farasapy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# df_AJGT[LABEL_COLUMN] = df_AJGT[LABEL_COLUMN].apply(lambda x: label_map[x])\u001b[39;00m\n\u001b[0;32m     15\u001b[0m train_AJGT, test_AJGT \u001b[38;5;241m=\u001b[39m train_test_split(data_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m data_train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [DATA_COLUMN, LABEL_COLUMN]\n\u001b[0;32m      7\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m }\n\u001b[1;32m---> 12\u001b[0m data_train[DATA_COLUMN] \u001b[38;5;241m=\u001b[39m data_train[DATA_COLUMN]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess(x, do_farasa_tokenization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m , farasa\u001b[38;5;241m=\u001b[39m\u001b[43mfarasa_segmenter\u001b[49m, use_farasapy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# df_AJGT[LABEL_COLUMN] = df_AJGT[LABEL_COLUMN].apply(lambda x: label_map[x])\u001b[39;00m\n\u001b[0;32m     15\u001b[0m train_AJGT, test_AJGT \u001b[38;5;241m=\u001b[39m train_test_split(data_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'farasa_segmenter' is not defined"
     ]
    }
   ],
   "source": [
    "train_AJGT, test_AJGT = train_test_split(data_train, test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed672e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
